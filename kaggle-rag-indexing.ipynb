{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10870648,"sourceType":"datasetVersion","datasetId":6753645},{"sourceId":11045772,"sourceType":"datasetVersion","datasetId":6880672}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cyber Threat Intelligence – RAG Indexing Notebook (Kaggle)\n\n**Purpose:** Data prep, chunking, embedding, and FAISS index building for the Voice-Enabled Cyber Threat Intelligence Assistant.\n\n**Use this notebook on Kaggle for:**\n- Downloading CVE / NVD or MITRE ATT&CK data\n- Chunking and embedding documents\n- Building and saving a FAISS index (+ metadata)\n- Quick retrieval experiments\n\n**Output:** Save the index and chunk metadata so your local Streamlit app can load them.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup & Dependencies\n\nRun once. On Kaggle, enable **GPU** (Settings → Accelerator → GPU) if you use a larger embedding model or Whisper later.","metadata":{}},{"cell_type":"code","source":"# Install packages (run this cell first)\n# faiss-gpu often has no pip wheel on many environments; faiss-cpu works everywhere\n!pip install -q faiss-cpu sentence-transformers anthropic  # anthropic = Claude API client","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:21.453745Z","iopub.execute_input":"2026-02-10T14:18:21.454118Z","iopub.status.idle":"2026-02-10T14:18:25.713529Z","shell.execute_reply.started":"2026-02-10T14:18:21.454090Z","shell.execute_reply":"2026-02-10T14:18:25.711842Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport requests\n\n# Chunking & embeddings\nfrom sentence_transformers import SentenceTransformer\nimport faiss\n\nprint(\"Setup OK\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:25.716061Z","iopub.execute_input":"2026-02-10T14:18:25.716384Z","iopub.status.idle":"2026-02-10T14:18:25.723331Z","shell.execute_reply.started":"2026-02-10T14:18:25.716351Z","shell.execute_reply":"2026-02-10T14:18:25.722134Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Setup OK\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"## 2. Configuration\n\nTo keep **embedding fast**: **max_docs** caps how many docs are indexed (default 5000); use a small **embedding_model** (e.g. L3) and **embedding_batch_size** 128. Set `max_docs: None` only when you need the full dataset.","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    \"chunk_size\": 384,\n    \"chunk_overlap\": 64,\n    \"chunk_mode\": \"doc\",       # \"doc\" = 1 chunk per document; or \"sentence\" / \"char\" for real chunking\n    \"max_docs\": 5000,           # cap so embedding stays fast; set None to index everything\n    \"sample_mode\": \"first\",     # \"first\" or \"random\" (uses sample_seed)\n    \"sample_seed\": 42,\n    \"embedding_model\": \"sentence-transformers/paraphrase-MiniLM-L3-v2\",  # L3 = faster/smaller; or all-MiniLM-L6-v2\n    \"embedding_batch_size\": 128,  # larger = faster on GPU; reduce to 32 if OOM\n    \"top_k\": 5,\n    \"index_dir\": \"/kaggle/working/rag_index\",\n    \"rebuild_anyway\": False,\n}\n\nPath(CONFIG[\"index_dir\"]).mkdir(parents=True, exist_ok=True)\nprint(\"Config:\", CONFIG)","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:25.724490Z","iopub.execute_input":"2026-02-10T14:18:25.724861Z","iopub.status.idle":"2026-02-10T14:18:25.746488Z","shell.execute_reply.started":"2026-02-10T14:18:25.724834Z","shell.execute_reply":"2026-02-10T14:18:25.745504Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Config: {'chunk_size': 384, 'chunk_overlap': 64, 'chunk_mode': 'doc', 'max_docs': 5000, 'sample_mode': 'first', 'sample_seed': 42, 'embedding_model': 'sentence-transformers/paraphrase-MiniLM-L3-v2', 'embedding_batch_size': 128, 'top_k': 5, 'index_dir': '/kaggle/working/rag_index', 'rebuild_anyway': False}\n","output_type":"stream"}],"execution_count":83},{"cell_type":"markdown","source":"## 3. Data Loading\n\nTwo cells only:\n1. **Default loader** – NVD CVE/CPE Kaggle dataset (attach it in **Add Data**).\n2. **Optional** – Add more sources (NVD API, MITRE ATT&CK, other). Uncomment what you need.","metadata":{}},{"cell_type":"markdown","source":"### 3a. Helper: Build list of documents (text + optional metadata)\n\nEach doc = `{\"text\": \"...\", \"source\": \"...\", \"id\": \"...\"}`.","metadata":{}},{"cell_type":"code","source":"def doc_to_text(doc: dict) -> str:\n    \"\"\"Single document to a single string for chunking.\"\"\"\n    parts = []\n    if doc.get(\"id\"):\n        parts.append(f\"ID: {doc['id']}\")\n    if doc.get(\"title\"):\n        parts.append(f\"Title: {doc['title']}\")\n    if doc.get(\"description\"):\n        parts.append(str(doc[\"description\"]))\n    if doc.get(\"text\"):\n        parts.append(str(doc[\"text\"]))\n    return \"\\n\".join(parts) if parts else \"\"\n\ndef normalize_text(s: str) -> str:\n    if not isinstance(s, str):\n        return \"\"\n    return \" \".join(s.split())\n\nprint(\"Helpers defined.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:25.748862Z","iopub.execute_input":"2026-02-10T14:18:25.749259Z","iopub.status.idle":"2026-02-10T14:18:25.764321Z","shell.execute_reply.started":"2026-02-10T14:18:25.749176Z","shell.execute_reply":"2026-02-10T14:18:25.763210Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Helpers defined.\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"### Default loader – NVD CVE/CPE Kaggle dataset\n\nAttach the dataset in **Add Data**: [NVD CVE/CPE till Feb 2025](https://www.kaggle.com/datasets/nikhilarora1729/nvd-cve-cpe-dataset-till-february-2025). If not attached, this cell leaves `documents` empty and you can use the optional cell below.","metadata":{}},{"cell_type":"code","source":"documents = []\nNVD_KAGGLE_PATH = Path(\"/kaggle/input/nvd-cve-cpe-dataset-till-february-2025\")\n\nif not NVD_KAGGLE_PATH.exists():\n    print(\"NVD CVE/CPE Kaggle dataset not attached. Run the optional cell below to use NVD API / MITRE ATT&CK instead.\")\nelse:\n    def _guess_cve_cols(df: pd.DataFrame):\n        cols = list(df.columns)\n        id_col = None\n        for c in cols:\n            cl = c.lower()\n            if \"cve\" in cl and (\"id\" in cl or cl.endswith(\"_id\") or cl == \"cve\"):\n                id_col = c\n                break\n        if id_col is None:\n            for c in cols:\n                if c.lower() in {\"cve_id\", \"cveid\"}:\n                    id_col = c\n                    break\n        desc_col = None\n        for c in cols:\n            cl = c.lower()\n            if any(k in cl for k in [\"description\", \"summary\", \"details\"]):\n                desc_col = c\n                break\n        return id_col, desc_col\n\n    added = 0\n    for csv_file in NVD_KAGGLE_PATH.glob(\"*.csv\"):\n        print(f\"Loading {csv_file.name} ...\")\n        df_nvd = pd.read_csv(csv_file)\n        id_col, desc_col = _guess_cve_cols(df_nvd)\n        print(f\"  id_col={id_col}, desc_col={desc_col}\")\n        if not id_col or not desc_col:\n            continue\n        for _, row in df_nvd.iterrows():\n            doc_text = row.get(desc_col, \"\")\n            if not isinstance(doc_text, str) or not doc_text.strip():\n                continue\n            documents.append({\n                \"id\": row.get(id_col, \"\"),\n                \"description\": doc_text,\n                \"source\": \"Kaggle_NVD_CVE_CPE\",\n            })\n            added += 1\n    print(f\"Added {added} documents. Total: {len(documents)}\")\n\ndocuments = [d for d in documents if normalize_text(doc_to_text(d))]\nprint(f\"Total documents to index: {len(documents)}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:25.765475Z","iopub.execute_input":"2026-02-10T14:18:25.765965Z","iopub.status.idle":"2026-02-10T14:18:45.169560Z","shell.execute_reply.started":"2026-02-10T14:18:25.765926Z","shell.execute_reply":"2026-02-10T14:18:45.168679Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading cpe.csv ...\n  id_col=None, desc_col=None\nLoading junction.csv ...\n  id_col=cveId, desc_col=None\nLoading nvd_cves.csv ...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/1107031847.py:31: DtypeWarning: Columns (17,18,19,20,21,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_nvd = pd.read_csv(csv_file)\n","output_type":"stream"},{"name":"stdout","text":"  id_col=cveId, desc_col=description\nAdded 282250 documents. Total: 282250\nTotal documents to index: 282250\n","output_type":"stream"},{"name":"stdout","output_type":"stream","text":["  id_col=cveId, desc_col=description\n","Added 282250 documents. Total: 282250\n","Total documents to index: 282250\n"]},{"name":"stdout","output_type":"stream","text":["  id_col=cveId, desc_col=description\n","Added 282250 documents. Total: 282250\n","Total documents to index: 282250\n"]}],"execution_count":85},{"cell_type":"markdown","source":"### Optional: add more sources\n\nUncomment the blocks you want: **NVD API** (recent CVEs), **MITRE ATT&CK** (techniques), or other Kaggle/Hugging Face datasets. Run this cell after the default loader.","metadata":{}},{"cell_type":"code","source":"def fetch_nvd_recent(results_per_page: int = 100) -> list:\n    \"\"\"Fetch recent CVEs from NVD API (no key). Rate limit ~5 req/30s.\"\"\"\n    url = \"https://services.nvd.nist.gov/rest/json/cves/2.0\"\n    params = {\"resultsPerPage\": results_per_page, \"startIndex\": 0}\n    out = []\n    try:\n        r = requests.get(url, params=params, timeout=30)\n        r.raise_for_status()\n        data = r.json()\n        for item in data.get(\"vulnerabilities\", []):\n            cve = item.get(\"cve\", {})\n            desc = (cve.get(\"descriptions\") or [{}])[0].get(\"value\", \"\")\n            out.append({\"id\": cve.get(\"id\", \"\"), \"title\": cve.get(\"id\", \"\"), \"description\": desc, \"source\": \"NVD\"})\n    except Exception as e:\n        print(\"NVD fetch error:\", e)\n    return out\n\ndef fetch_mitre_attack_enterprise() -> list:\n    \"\"\"Load MITRE ATT&CK Enterprise techniques from official STIX JSON.\"\"\"\n    url = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n    out = []\n    try:\n        r = requests.get(url, timeout=60)\n        r.raise_for_status()\n        data = r.json()\n        for obj in data.get(\"objects\", []):\n            if obj.get(\"type\") != \"attack-pattern\":\n                continue\n            name = obj.get(\"name\", \"\")\n            desc = obj.get(\"description\", \"\")\n            ext = obj.get(\"external_references\", [])\n            refs = \" \".join([e.get(\"external_id\", \"\") for e in ext if e.get(\"external_id\")])\n            if ext:\n                ext_id = ext[0].get(\"external_id\", obj.get(\"id\", \"\"))\n            else:\n                ext_id = obj.get(\"id\", \"\")\n            out.append({\n                \"id\": ext_id,\n                \"title\": name,\n                \"description\": desc,\n                \"text\": refs,\n                \"source\": \"MITRE_ATTACK\",\n            })\n    except Exception as e:\n        print(\"MITRE fetch error:\", e)\n    return out\n\n# --- Uncomment what you need ---\n# nvd_docs = fetch_nvd_recent(results_per_page=200)\n# documents.extend(nvd_docs)\n# print(f\"After NVD API: {len(documents)}\")\n\n# attack_docs = fetch_mitre_attack_enterprise()\n# documents.extend(attack_docs)\n# print(f\"After MITRE ATT&CK: {len(documents)}\")\n\n# === Global Cybersecurity Threats 2015–2024 (Kaggle) ===\n# Dataset: https://www.kaggle.com/datasets/atharvasoundankar/global-cybersecurity-threats-2015-2024\n# After attaching it in \"Add Data\", the path will look like:\n#   /kaggle/input/global-cybersecurity-threats-2015-2024\n\n# NVD API (optional)\n# nvd_docs = fetch_nvd_recent(results_per_page=200)\n# documents.extend(nvd_docs)\n# print(f\"After NVD API: {len(documents)}\")\n\n# MITRE ATT&CK (enable this to enrich responses)\nattack_docs = fetch_mitre_attack_enterprise()\ndocuments.extend(attack_docs)\nprint(f\"After MITRE ATT&CK: {len(attack_docs)} techniques. Total documents: {len(documents)}\")\n\n\nGCT_PATH = \"/kaggle/input//kaggle/input/datasets/atharvasoundankar/global-cybersecurity-threats-2015-2024\"\nif Path(GCT_PATH).exists():\n    try:\n        # Check the actual filename in Kaggle's Data tab; adjust if needed\n        df_gct = pd.read_csv(f\"{GCT_PATH}/global_cybersecurity_threats.csv\")\n    except Exception as e:\n        print(\"Could not load Global Cybersecurity Threats dataset:\", e)\n    else:\n        # print(df_gct.columns)  # uncomment once to inspect columns\n        def build_threat_text(row):\n            parts = []\n            year = row.get(\"Year\") or row.get(\"year\")\n            country = row.get(\"Country\") or row.get(\"country\")\n            attack_type = row.get(\"Attack_Type\") or row.get(\"Attack Type\") or row.get(\"attack_type\")\n            sector = row.get(\"Target_Industry\") or row.get(\"Industry\") or row.get(\"Target Industry\")\n            loss = row.get(\"Financial_Loss_Million_USD\") or row.get(\"Financial_Loss\") or row.get(\"financial_loss\")\n            users = row.get(\"Affected_Users_Million\") or row.get(\"Affected_Users\") or row.get(\"affected_users\")\n            vuln = row.get(\"Vulnerability_Exploited\") or row.get(\"Vulnerability\")\n            group = row.get(\"Attack_Source\") or row.get(\"Source\") or row.get(\"Attacker\")\n\n            if year or country:\n                parts.append(f\"In {year} in {country},\")\n            if attack_type:\n                parts.append(f\"a {attack_type} attack\")\n            if sector:\n                parts.append(f\"targeted the {sector} sector\")\n            sentence = \" \".join(p for p in parts if p).strip()\n            if sentence and not sentence.endswith(\".\"):\n                sentence += \".\"\n\n            details = []\n            if loss not in (None, \"\"):\n                details.append(f\"Estimated financial loss: {loss}.\")\n            if users not in (None, \"\"):\n                details.append(f\"Affected users (millions): {users}.\")\n            if vuln:\n                details.append(f\"Vulnerability exploited: {vuln}.\")\n            if group:\n                details.append(f\"Attack source: {group}.\")\n\n            text = \" \".join([sentence] + details).strip()\n            return text\n\n        added_gct = 0\n        for idx, row in df_gct.iterrows():\n            text = build_threat_text(row)\n            if not text:\n                continue\n            documents.append({\n                \"id\": f\"GCT-{idx}\",\n                \"title\": row.get(\"Attack_Type\", row.get(\"attack_type\", \"\")),\n                \"description\": text,\n                \"source\": \"Kaggle_GlobalThreats\",\n            })\n            added_gct += 1\n        print(f\"Added {added_gct} Global Cybersecurity Threat incidents. Total documents: {len(documents)}\")\nelse:\n    print(\"Global Cybersecurity Threats dataset not attached; skipping.\")\n\ndocuments = [d for d in documents if normalize_text(doc_to_text(d))]\nprint(f\"Total documents to index: {len(documents)}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:45.170800Z","iopub.execute_input":"2026-02-10T14:18:45.171119Z","iopub.status.idle":"2026-02-10T14:18:47.608086Z","shell.execute_reply.started":"2026-02-10T14:18:45.171093Z","shell.execute_reply":"2026-02-10T14:18:47.606958Z"},"trusted":true},"outputs":[{"name":"stdout","text":"After MITRE ATT&CK: 835 techniques. Total documents: 283085\nGlobal Cybersecurity Threats dataset not attached; skipping.\nTotal documents to index: 283085\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# Documents are filled by the default loader and optional cell above.\n# Proceed to chunking (section 4).","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:47.609111Z","iopub.execute_input":"2026-02-10T14:18:47.609520Z","iopub.status.idle":"2026-02-10T14:18:47.613748Z","shell.execute_reply.started":"2026-02-10T14:18:47.609490Z","shell.execute_reply":"2026-02-10T14:18:47.612748Z"},"trusted":true},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":"## 4. Chunking\n\nSplit each document into overlapping chunks. Use **chunk_mode**: `\"sentence\"` to avoid cutting mid-sentence, or `\"char\"` for character-based. Set **max_docs** in CONFIG to cap documents for quick runs.","metadata":{}},{"cell_type":"code","source":"import re\nimport random\n\ndef chunk_text(text: str, chunk_size: int, overlap: int, mode: str = \"char\") -> list:\n    \"\"\"Split into chunks. mode='char': character-based. mode='sentence': group by sentences to avoid mid-sentence cuts.\"\"\"\n    if not text or chunk_size <= 0:\n        return []\n    text = normalize_text(text)\n    if mode == \"sentence\":\n        # Split on sentence boundaries, then merge into ~chunk_size\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        sentences = [s.strip() for s in sentences if s.strip()]\n        chunks = []\n        current, current_len = [], 0\n        for s in sentences:\n            if current_len + len(s) + 1 <= chunk_size:\n                current.append(s)\n                current_len += len(s) + 1\n            else:\n                if current:\n                    chunks.append(\" \".join(current))\n                # overlap: keep last sentence(s) that fit in overlap\n                overlap_len = 0\n                overlap_sentences = []\n                for x in reversed(current):\n                    if overlap_len + len(x) + 1 <= overlap:\n                        overlap_sentences.insert(0, x)\n                        overlap_len += len(x) + 1\n                    else:\n                        break\n                current = overlap_sentences + [s] if overlap else [s]\n                current_len = sum(len(x) for x in current) + len(current) - 1\n        if current:\n            chunks.append(\" \".join(current))\n        return chunks\n    # character-based (original)\n    step = max(1, chunk_size - overlap)\n    chunks = []\n    for start in range(0, len(text), step):\n        c = text[start : start + chunk_size]\n        if c:\n            chunks.append(c)\n        if start + chunk_size >= len(text):\n            break\n    return chunks\n\ndef build_chunks(documents: list, config: dict) -> tuple:\n    \"\"\"Return (list of chunk strings, list of metadata dicts). Uses max_docs and sample_mode (first/random).\"\"\"\n    chunks, meta = [], []\n    docs = list(documents)\n    if config.get(\"max_docs\"):\n        if config.get(\"sample_mode\") == \"random\":\n            random.seed(config.get(\"sample_seed\", 42))\n            random.shuffle(docs)\n        docs = docs[: config[\"max_docs\"]]\n    for doc in docs:\n        text = doc_to_text(doc)\n        if not text:\n            continue\n        for c in chunk_text(\n            text,\n            config[\"chunk_size\"],\n            config[\"chunk_overlap\"],\n            config.get(\"chunk_mode\", \"char\"),\n        ):\n            chunks.append(c)\n            meta.append({\n                \"source\": doc.get(\"source\", \"\"),\n                \"doc_id\": doc.get(\"id\", \"\"),\n                \"title\": doc.get(\"title\", \"\"),\n            })\n    return chunks, meta\n\nchunk_texts, chunk_meta = build_chunks(documents, CONFIG)\nprint(f\"Total chunks: {len(chunk_texts)} (chunk_mode={CONFIG.get('chunk_mode', 'char')})\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:47.614919Z","iopub.execute_input":"2026-02-10T14:18:47.615311Z","iopub.status.idle":"2026-02-10T14:18:47.678039Z","shell.execute_reply.started":"2026-02-10T14:18:47.615282Z","shell.execute_reply":"2026-02-10T14:18:47.677264Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total chunks: 5533 (chunk_mode=doc)\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"# 5.1 BM25 index over chunks (for hybrid retrieval)\n!pip install -q rank_bm25 nltk\n\nfrom rank_bm25 import BM25Okapi\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Download tokenizer data quietly (first run only)\nnltk.download(\"punkt\", quiet=True)\n\n# Build BM25 index over the same chunks used for embeddings\ntokenized_chunks = [word_tokenize(c.lower()) for c in chunk_texts]\nbm25 = BM25Okapi(tokenized_chunks)\nprint(\"BM25 index built over\", len(tokenized_chunks), \"chunks\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T14:18:47.679096Z","iopub.execute_input":"2026-02-10T14:18:47.679459Z","iopub.status.idle":"2026-02-10T14:18:52.718520Z","shell.execute_reply.started":"2026-02-10T14:18:47.679419Z","shell.execute_reply":"2026-02-10T14:18:52.717147Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"BM25 index built over 5533 chunks\n","output_type":"stream"}],"execution_count":89},{"cell_type":"markdown","source":"## 5. Embeddings & FAISS Index\n\nEmbed all chunks and build FAISS index. If **rebuild_anyway** is False and an existing index exists with matching config and chunk count, we load it and skip rebuild.","metadata":{}},{"cell_type":"code","source":"index_dir = CONFIG[\"index_dir\"]\nconfig_path = os.path.join(index_dir, \"config.json\")\nindex_path = os.path.join(index_dir, \"faiss.index\")\nINDEX_WAS_LOADED = False\n\nif (\n    not CONFIG.get(\"rebuild_anyway\", False)\n    and os.path.exists(config_path)\n    and os.path.exists(index_path)\n):\n    with open(config_path, encoding=\"utf-8\") as f:\n        old = json.load(f)\n    n_chunks = len(chunk_texts)\n    if (\n        old.get(\"embedding_model\") == CONFIG[\"embedding_model\"]\n        and old.get(\"top_k\") == CONFIG[\"top_k\"]\n        and old.get(\"ntotal\") == n_chunks\n    ):\n        index = faiss.read_index(index_path)\n        dim = int(old[\"dim\"])\n        model = SentenceTransformer(CONFIG[\"embedding_model\"])\n        INDEX_WAS_LOADED = True\n        print(f\"Loaded existing index (ntotal={index.ntotal}, dim={dim}). Skipped rebuild.\")\n    else:\n        old = None\nif not INDEX_WAS_LOADED:\n    model = SentenceTransformer(CONFIG[\"embedding_model\"])\n    batch_size = CONFIG.get(\"embedding_batch_size\", 128)\n    embeddings = model.encode(chunk_texts, show_progress_bar=True, batch_size=batch_size)\n    embeddings = np.array(embeddings, dtype=np.float32)\n    dim = embeddings.shape[1]\n    print(f\"Embeddings shape: {embeddings.shape}, dim={dim}\")\n    index = faiss.IndexFlatIP(dim)\n    faiss.normalize_L2(embeddings)\n    index.add(embeddings)\n    print(f\"FAISS index: {index.ntotal} vectors.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:52.723688Z","iopub.execute_input":"2026-02-10T14:18:52.724125Z","iopub.status.idle":"2026-02-10T14:18:56.683955Z","shell.execute_reply.started":"2026-02-10T14:18:52.724089Z","shell.execute_reply":"2026-02-10T14:18:56.682931Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loaded existing index (ntotal=5533, dim=384). Skipped rebuild.\n","output_type":"stream"}],"execution_count":90},{"cell_type":"markdown","source":"## 6. Save Index & Metadata\n\nPersist under `/kaggle/working/` so you can download or add as output dataset.","metadata":{}},{"cell_type":"code","source":"if not INDEX_WAS_LOADED:\n    index_dir = CONFIG[\"index_dir\"]\n    faiss.write_index(index, os.path.join(index_dir, \"faiss.index\"))\n    with open(os.path.join(index_dir, \"chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(chunk_texts, f, ensure_ascii=False, indent=0)\n    with open(os.path.join(index_dir, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(chunk_meta, f, ensure_ascii=False, indent=0)\n    with open(os.path.join(index_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump({\n            \"embedding_model\": CONFIG[\"embedding_model\"],\n            \"top_k\": CONFIG[\"top_k\"],\n            \"dim\": dim,\n            \"ntotal\": index.ntotal,\n        }, f, indent=2)\n    print(\"Saved:\", os.listdir(index_dir))\nelse:\n    print(\"Index was loaded; skip save.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:56.685243Z","iopub.execute_input":"2026-02-10T14:18:56.685686Z","iopub.status.idle":"2026-02-10T14:18:56.693943Z","shell.execute_reply.started":"2026-02-10T14:18:56.685654Z","shell.execute_reply":"2026-02-10T14:18:56.693083Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Index was loaded; skip save.\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"## 7. Retrieval Experiment\n\nTest retrieval with a few queries. Use the same model and normalization as at index time.","metadata":{}},{"cell_type":"code","source":"# Hybrid retrieval: Reciprocal Rank Fusion (RRF) over FAISS + BM25\n\ndef hybrid_search_rrf(\n    query: str,\n    index,\n    chunk_texts,\n    chunk_meta,\n    model,\n    top_k: int = 5,\n    faiss_k: int = 50,\n    bm25_k: int = 50,\n    k_rrf: int = 60,\n):\n    \"\"\"Combine FAISS (dense) and BM25 (lexical) rankings using Reciprocal Rank Fusion.\n\n    fused_score = 1 / (k_rrf + rank_faiss) + 1 / (k_rrf + rank_bm25)\n    \"\"\"\n    # 1) FAISS ranking (dense)\n    faiss_results = search(query, index, chunk_texts, model, top_k=faiss_k)\n    faiss_ranks = {r[\"idx\"]: rank for rank, r in enumerate(faiss_results)}\n\n    # 2) BM25 ranking (lexical)\n    q_tokens = word_tokenize(query.lower())\n    bm25_scores = bm25.get_scores(q_tokens)\n    bm25_top_idx = np.argsort(bm25_scores)[::-1][:bm25_k]\n    bm25_ranks = {int(idx): rank for rank, idx in enumerate(bm25_top_idx)}\n\n    # 3) RRF fusion over union of candidates\n    all_ids = set(faiss_ranks.keys()) | set(bm25_ranks.keys())\n    fused = []\n    for cid in all_ids:\n        r_faiss = faiss_ranks.get(cid)\n        r_bm25 = bm25_ranks.get(cid)\n        score = 0.0\n        if r_faiss is not None:\n            score += 1.0 / (k_rrf + r_faiss)\n        if r_bm25 is not None:\n            score += 1.0 / (k_rrf + r_bm25)\n        fused.append((cid, score))\n\n    # 4) Rank fused results\n    fused.sort(key=lambda x: x[1], reverse=True)\n    results = []\n    for cid, score in fused[:top_k]:\n        results.append(\n            {\n                \"idx\": int(cid),\n                \"chunk\": chunk_texts[cid],\n                \"score\": float(score),\n                \"source\": chunk_meta[cid].get(\"source\", \"\"),\n            }\n        )\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T14:18:56.695115Z","iopub.execute_input":"2026-02-10T14:18:56.695489Z","iopub.status.idle":"2026-02-10T14:18:56.717680Z","shell.execute_reply.started":"2026-02-10T14:18:56.695439Z","shell.execute_reply":"2026-02-10T14:18:56.716316Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"def search(query: str, index, chunk_texts: list, model, top_k: int = 5):\n    q_emb = model.encode([query])\n    q_emb = np.array(q_emb, dtype=np.float32)\n    faiss.normalize_L2(q_emb)\n    scores, indices = index.search(q_emb, min(top_k, index.ntotal))\n    return [\n        {\"chunk\": chunk_texts[i], \"score\": float(s), \"idx\": int(i)}\n        for i, s in zip(indices[0], scores[0])\n    ]\n\nfor q in [\"Windows vulnerability\", \"remote code execution\", \"ransomware\"]:\n    results = search(q, index, chunk_texts, model, CONFIG[\"top_k\"])\n    print(f\"Query: {q}\")\n    for r in results[:2]:\n        print(f\"  score={r['score']:.3f} | {r['chunk'][:120]}...\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:56.721178Z","iopub.execute_input":"2026-02-10T14:18:56.721686Z","iopub.status.idle":"2026-02-10T14:18:56.772870Z","shell.execute_reply.started":"2026-02-10T14:18:56.721626Z","shell.execute_reply":"2026-02-10T14:18:56.771966Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Query: Windows vulnerability\n  score=0.591 | emote attackers to execute arbitrary code on Windows by leveraging an untrusted search path vulnerability in (a) Interne...\n  score=0.587 |  As of 20081210, it is unclear whether this vulnerability is related to a WordPad issue disclosed on 20080925 with a 200...\n\nQuery: remote code execution\n  score=0.639 | leveraged for arbitrary remote code execution in conjunction with CVE-2007-6378....\n  score=0.593 | R01 allows remote attackers to execute arbitrary code via a long Session cookie....\n\nQuery: ransomware\n  score=0.507 | ID: CVE-2008-0792 Multiple F-Secure anti-virus products, including Internet Security 2006 through 2008, Anti-Virus 2006 ...\n  score=0.489 |  can be leveraged for attacks such as DNS cache poisoning against OpenBSD's modification of BIND....\n\n","output_type":"stream"}],"execution_count":93},{"cell_type":"markdown","source":"### 7a. RAG prompt template\n\nRAG needs a **prompt** that gives the LLM the retrieved context and the user question. Use this template in the notebook and in your Streamlit app so answers are grounded in the retrieved chunks.","metadata":{}},{"cell_type":"code","source":"# RAG prompt: system instruction + user message with {context} and {question}\nRAG_SYSTEM_PROMPT = \"\"\"You are a cybersecurity threat intelligence assistant. \nAnswer only from the provided context. If the context does not contain enough information,\nsay so. Be concise and cite CVE IDs or technique names when relevant.You may see MITRE ATT&CK techniques under ‘MITRE ATT&CK techniques’. \nWhen mapping attacks, prefer these techniques and do not invent techniques that are not in the context.\"\"\"\n\nRAG_USER_TEMPLATE = \"\"\"Context (from threat intelligence documents):\n\n{context}\n\nQuestion: {question}\n\nAnswer briefly and based only on the context above:\"\"\"\n\ndef build_rag_prompt(context: str, question: str, max_context_chars: int = 6000) -> str:\n    \"\"\"Build the user prompt for RAG: context + question. Truncate context if needed.\"\"\"\n    context_trimmed = context[:max_context_chars] if len(context) > max_context_chars else context\n    return RAG_USER_TEMPLATE.format(context=context_trimmed, question=question)\n\nprint(\"RAG prompt template defined. Use build_rag_prompt(context, question) for queries.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:56.773912Z","iopub.execute_input":"2026-02-10T14:18:56.774224Z","iopub.status.idle":"2026-02-10T14:18:56.781244Z","shell.execute_reply.started":"2026-02-10T14:18:56.774157Z","shell.execute_reply":"2026-02-10T14:18:56.780276Z"},"trusted":true},"outputs":[{"name":"stdout","text":"RAG prompt template defined. Use build_rag_prompt(context, question) for queries.\n","output_type":"stream"}],"execution_count":94},{"cell_type":"markdown","source":"### 7b. Retrieval evaluation (optional)\n\nChecks whether retrieval finds relevant docs: for each (query, expected_doc_id_substring), we see if any retrieved chunk's `doc_id` contains that string. Reports **Hit@k** (was expected doc in top-k?) and **MRR** (mean reciprocal rank).","metadata":{}},{"cell_type":"code","source":"# Eval set: list of (query, expected_doc_id_substring). Edit with real CVE/technique IDs from your data.\nEVAL_QUERIES = [\n    (\"Windows remote code execution vulnerability\", \"CVE-\"),   # any CVE\n    (\"privilege escalation\", \"CVE-\"),\n    (\"cross-site scripting\", \"CVE-\"),\n]\n\ndef eval_retrieval(queries, index, chunk_texts, chunk_meta, model, top_k=5):\n    hit = 0\n    rr_sum = 0.0\n    for query, expected_substr in queries:\n        results = search(query, index, chunk_texts, model, top_k)\n        doc_ids = [chunk_meta[r[\"idx\"]].get(\"doc_id\", \"\") for r in results]\n        found_rank = None\n        for i, did in enumerate(doc_ids):\n            if expected_substr in str(did):\n                found_rank = i + 1\n                break\n        if found_rank is not None:\n            hit += 1\n            rr_sum += 1.0 / found_rank\n    n = len(queries)\n    hit_at_k = hit / n if n else 0\n    mrr = rr_sum / n if n else 0\n    return hit_at_k, mrr\n\nif EVAL_QUERIES and chunk_texts:\n    hit_at_k, mrr = eval_retrieval(EVAL_QUERIES, index, chunk_texts, chunk_meta, model, CONFIG[\"top_k\"])\n    print(f\"Hit@{CONFIG['top_k']}: {hit_at_k:.2%}  |  MRR: {mrr:.3f}\")\nelse:\n    print(\"Add (query, expected_doc_id_substring) to EVAL_QUERIES and re-run to get Hit@k and MRR.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:56.782775Z","iopub.execute_input":"2026-02-10T14:18:56.783269Z","iopub.status.idle":"2026-02-10T14:18:56.830642Z","shell.execute_reply.started":"2026-02-10T14:18:56.783187Z","shell.execute_reply":"2026-02-10T14:18:56.829745Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Hit@5: 100.00%  |  MRR: 1.000\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"# Anthropic API key: use env ANTHROPIC_API_KEY, or paste below for this run (do not commit after).\nimport os\nANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")  # e.g. \"sk-ant-api03-...\"\nif ANTHROPIC_API_KEY:\n    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n    print(\"ANTHROPIC_API_KEY set.\")\nelse:\n    print(\"Paste your key in ANTHROPIC_API_KEY above, or set it in Kaggle Secrets.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:56.831824Z","iopub.execute_input":"2026-02-10T14:18:56.832294Z","iopub.status.idle":"2026-02-10T14:18:56.837368Z","shell.execute_reply.started":"2026-02-10T14:18:56.832247Z","shell.execute_reply":"2026-02-10T14:18:56.836423Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Paste your key in ANTHROPIC_API_KEY above, or set it in Kaggle Secrets.\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"!pip install -q openai\n","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:18:56.838282Z","iopub.execute_input":"2026-02-10T14:18:56.838540Z","iopub.status.idle":"2026-02-10T14:19:00.816446Z","shell.execute_reply.started":"2026-02-10T14:18:56.838514Z","shell.execute_reply":"2026-02-10T14:19:00.815289Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"# Groq key setup for llama-3.1-8b-instant\nimport os\n\n# Prefer setting GROQ_API_KEY in environment / Kaggle Secrets.\n# The default here is for local experimentation only – REMOVE before sharing.\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\nGROQ_BASE_URL = os.environ.get(\"GROQ_BASE_URL\", \"https://api.groq.com/openai/v1\")\n\nif GROQ_API_KEY:\n    # The OpenAI client below reads these env vars, so we map Groq -> OpenAI_* names\n    os.environ[\"OPENAI_API_KEY\"] = GROQ_API_KEY\n    os.environ[\"OPENAI_BASE_URL\"] = GROQ_BASE_URL\n    print(\"Groq key set. Base URL:\", GROQ_BASE_URL)\nelse:\n    print(\"Set GROQ_API_KEY in env / Kaggle Secrets or paste it above.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:19:00.818049Z","iopub.execute_input":"2026-02-10T14:19:00.818554Z","iopub.status.idle":"2026-02-10T14:19:00.826283Z","shell.execute_reply.started":"2026-02-10T14:19:00.818516Z","shell.execute_reply":"2026-02-10T14:19:00.824946Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Groq key set. Base URL: https://api.groq.com/openai/v1\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"from openai import OpenAI\n\nGROQ_MODEL = \"llama-3.1-8b-instant\"  # Groq model ID\n\nLLM_QUESTIONS = [\n    \"What are the main risks described in the context? Summarize in 2-3 bullet points.\",\n    \"List any CVE or vulnerability IDs mentioned in the context.\",\n]\n\ndef rag_llm_check_openrouter(questions, index, chunk_texts, chunk_meta, model, config):\n    import os\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    base_url = os.environ.get(\"OPENAI_BASE_URL\", \"https://api.groq.com/openai/v1\")\n    if not api_key:\n        print(\"Set GROQ_API_KEY (your Groq key) before running this cell.\")\n        return\n\n    client = OpenAI(api_key=api_key, base_url=base_url)\n\n    for q in questions[:2]:\n        # 1) RAG retrieval\n        results = search(q, index, chunk_texts, model, config[\"top_k\"])\n\n        # Split context into: vulnerabilities/incidents vs MITRE ATT&CK techniques\n        vuln_chunks = []\n        mitre_chunks = []\n        for r in results:\n            meta = chunk_meta[r[\"idx\"]]\n            src = meta.get(\"source\", \"\")\n            if src == \"MITRE_ATTACK\":\n                mitre_chunks.append(r[\"chunk\"])\n            else:\n                vuln_chunks.append(r[\"chunk\"])\n\n        # 2) Build structured context\n        parts = []\n        if vuln_chunks:\n            parts.append(\"Vulnerabilities and incidents:\\n\\n\" +\n                         \"\\n\\n---\\n\\n\".join(vuln_chunks))\n        if mitre_chunks:\n            parts.append(\"MITRE ATT&CK techniques:\\n\\n\" +\n                         \"\\n\\n---\\n\\n\".join(mitre_chunks))\n\n        context = \"\\n\\n\\n\".join(parts) if parts else \"\"\n        user_prompt = build_rag_prompt(context, q)\n\n        # 3) LLM call via Groq (OpenAI-compatible endpoint)\n        try:\n            resp = client.chat.completions.create(\n                model=GROQ_MODEL,\n                messages=[\n                    {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": user_prompt},\n                ],\n                max_tokens=400,\n                temperature=0.4,\n                top_p=0.9,\n                top_k=50,\n            )\n            answer = resp.choices[0].message.content\n            print(f\"Q: {q[:70]}...\")\n            print(f\"A: {answer}\\n\")\n        except Exception as e:\n            print(f\"Groq API error: {e}\")\n            print(\"Prompt preview:\", user_prompt[:300], \"...\\n\")\n            break\n\nif chunk_texts:\n    rag_llm_check_openrouter(LLM_QUESTIONS, index, chunk_texts, chunk_meta, model, CONFIG)\nelse:\n    print(\"No chunks; run data loading and chunking first.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:19:00.827684Z","iopub.execute_input":"2026-02-10T14:19:00.828137Z","iopub.status.idle":"2026-02-10T14:19:00.927428Z","shell.execute_reply.started":"2026-02-10T14:19:00.828108Z","shell.execute_reply":"2026-02-10T14:19:00.926413Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Groq API error: Completions.create() got an unexpected keyword argument 'top_k'\nPrompt preview: Context (from threat intelligence documents):\n\nVulnerabilities and incidents:\n\norun.inf file, and possibly other vectors related to (a) AutoRun and (b) AutoPlay actions.\n\n---\n\n NOTE: some of these details are obtained from third party information.\n\n---\n\n some of these details are obtained from third ...\n\n","output_type":"stream"}],"execution_count":99},{"cell_type":"markdown","source":"### 7c. Optional LLM check (validates full RAG chain)\n\nUses **Anthropic Claude API** (free credits when you sign up at [console.anthropic.com](https://console.anthropic.com)). Set **ANTHROPIC_API_KEY** in the environment (or Kaggle Secrets). Runs retrieval → build prompt → call Claude.","metadata":{}},{"cell_type":"code","source":"from openai import OpenAI\n\nGROQ_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"  # Groq model ID || Tool Use, JSON Object Mode, JSON Schema Mode, Vision\n\ndef ask_rag(question: str):\n    \"\"\"\n    Free-form question → RAG retrieval → llama-4-maverick-17b-128e-instruct via Groq.\n    Example question:\n      \"I clicked a link from an unknown person and now my PC won't start.\n       What kind of attack could this be and what should I do?\"\n    \"\"\"\n    import os\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    base_url = os.environ.get(\"OPENAI_BASE_URL\", \"https://api.groq.com/openai/v1\")\n    if not api_key:\n        print(\"Set GROQ_API_KEY (Groq key) first.\")\n        return\n\n    if not chunk_texts:\n        print(\"No chunks; run data loading, chunking, and indexing cells first.\")\n        return\n\n    client = OpenAI(api_key=api_key, base_url=base_url)\n\n    # 1) Retrieve relevant context from your threat index\n    results = search(question, index, chunk_texts, model, CONFIG[\"top_k\"])\n    context = \"\\n\\n---\\n\\n\".join([r[\"chunk\"] for r in results])\n\n    # 2) Build RAG prompt using your template\n    user_prompt = build_rag_prompt(context, question)\n\n    # 3) Call llama-3.1-8b-instant via Groq\n    try:\n        resp = client.chat.completions.create(\n            model=GROQ_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            max_tokens=500,\n            temperature=0.4,\n            top_p=0.9,\n        )\n        answer = resp.choices[0].message.content\n        print(\"Question:\")\n        print(question)\n        print(\"\\nAnswer:\")\n        print(answer)\n    except Exception as e:\n        print(f\"Groq API error: {e}\")\n        print(\"\\nPrompt preview:\\n\", user_prompt[:400], \"...\")","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:22:01.263041Z","iopub.execute_input":"2026-02-10T14:22:01.264033Z","iopub.status.idle":"2026-02-10T14:22:01.273383Z","shell.execute_reply.started":"2026-02-10T14:22:01.263989Z","shell.execute_reply":"2026-02-10T14:22:01.272333Z"},"trusted":true},"outputs":[],"execution_count":105},{"cell_type":"code","source":"# ✏️ Type your question here and run this cell, then run ask_rag(...)\nuser_question = \"\"\"\nI got a link from an unknown person. After clicking it and downloading a file,\nmy PC doesn’t start properly anymore. What kinds of attacks or malware could\nthis be, and what should I do next?\n\"\"\".strip()\n\nask_rag(user_question)","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:22:09.335402Z","iopub.execute_input":"2026-02-10T14:22:09.335787Z","iopub.status.idle":"2026-02-10T14:22:13.243370Z","shell.execute_reply.started":"2026-02-10T14:22:09.335756Z","shell.execute_reply":"2026-02-10T14:22:13.242486Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question:\nI got a link from an unknown person. After clicking it and downloading a file,\nmy PC doesn’t start properly anymore. What kinds of attacks or malware could\nthis be, and what should I do next?\n\nAnswer:\nBased on the context, the issue could be related to a denial of service (DoS) attack, potentially caused by a malformed archive or a crafted file that exploits a vulnerability in an antivirus engine, similar to CVE-2008-3447 or CVE-2008-1437/CVE-2008-1438. To proceed, you should: \n\n1. Disconnect from the internet to prevent further potential damage.\n2. Try to boot in safe mode or use a recovery disk to diagnose the issue.\n3. Run a thorough scan with an updated antivirus engine to detect potential malware.\n\nThe context does not provide enough information to determine the exact malware or attack. More information is needed for a precise diagnosis.\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"# Non-technical question (end user / manager)\nuser_question = \"\"\"\nI work in a small company and recently our employees received very realistic\nemails asking them to click a link to \"verify their account\". Some people\nclicked and entered their passwords. I don't know much about cybersecurity.\n\nBased on known cyber attacks and MITRE ATT&CK techniques, what kind of attack\nis this, what are the main risks for our company, and what immediate steps\nshould we take to reduce the damage?\n\"\"\".strip()\n\nask_rag(user_question)","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:22:22.518048Z","iopub.execute_input":"2026-02-10T14:22:22.518552Z","iopub.status.idle":"2026-02-10T14:22:24.669413Z","shell.execute_reply.started":"2026-02-10T14:22:22.518518Z","shell.execute_reply":"2026-02-10T14:22:24.668297Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question:\nI work in a small company and recently our employees received very realistic\nemails asking them to click a link to \"verify their account\". Some people\nclicked and entered their passwords. I don't know much about cybersecurity.\n\nBased on known cyber attacks and MITRE ATT&CK techniques, what kind of attack\nis this, what are the main risks for our company, and what immediate steps\nshould we take to reduce the damage?\n\nAnswer:\nThe attack you're describing is likely a phishing attack, which is not directly listed in the provided CVEs, but can be related to CVE-2008-3868 (CSRF) in the sense that both can be used to steal credentials. The main risk is that attackers may have obtained your employees' passwords, potentially gaining unauthorized access to your company's systems and data.\n\nImmediate steps to reduce the damage:\n\n1. Inform all employees about the phishing attack and instruct them not to click on suspicious links or enter their passwords on unknown websites.\n2. Force a password reset for all employees who clicked on the link and entered their passwords.\n3. Monitor your systems for any suspicious activity.\n\nMITRE ATT&CK techniques: \n- Phishing (T1566)\n- Credential Dumping or simply obtaining credentials through Phishing (T1056 is not directly related but T1566 is)\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# Technical question (security analyst / engineer)\nuser_question = \"\"\"\nWe suspect a targeted phishing campaign against our finance team. The initial\nvector appears to be email with malicious links, possibly corresponding to\nMITRE ATT&CK techniques T1566 (Phishing) and T1204 (User Execution).\n\nUsing the context and MITRE ATT&CK, can you:\n1. Identify the most relevant ATT&CK techniques for this scenario (initial access and execution)?\n2. Describe likely follow-on techniques (e.g., credential access, lateral movement) we should watch for.\n3. Recommend concrete detection and mitigation actions mapped to those techniques.\n\"\"\".strip()\n\nask_rag(user_question)","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:22:37.699514Z","iopub.execute_input":"2026-02-10T14:22:37.699824Z","iopub.status.idle":"2026-02-10T14:22:39.931954Z","shell.execute_reply.started":"2026-02-10T14:22:37.699798Z","shell.execute_reply":"2026-02-10T14:22:39.931064Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question:\nWe suspect a targeted phishing campaign against our finance team. The initial\nvector appears to be email with malicious links, possibly corresponding to\nMITRE ATT&CK techniques T1566 (Phishing) and T1204 (User Execution).\n\nUsing the context and MITRE ATT&CK, can you:\n1. Identify the most relevant ATT&CK techniques for this scenario (initial access and execution)?\n2. Describe likely follow-on techniques (e.g., credential access, lateral movement) we should watch for.\n3. Recommend concrete detection and mitigation actions mapped to those techniques.\n\nAnswer:\n1. The most relevant ATT&CK techniques for this scenario are T1566 (Phishing) for initial access and T1204 (User Execution) for execution, as the initial vector is email with malicious links.\n\n2. Likely follow-on techniques to watch for include T1056 (Keylogging or Input Capture) or T1111 (Two-Factor Authentication Interception) for credential access, and T1074 (Data Staging) or T1021 (Remote Desktop Protocol) for lateral movement, although the context does not directly mention these.\n\n   However, a more directly related technique from the given CVEs is open redirect (CVE-2008-4104), which can be used for phishing. No direct relation to the HP-UX CVEs is seen in the phishing context.\n\n3. Detection and mitigation actions:\n   - Monitor email traffic for suspicious links (T1566).\n   - Educate users not to click on suspicious links (T1204).\n   - Implement URL filtering or blocking of unknown URLs.\n   - Watch for open redirect vulnerabilities in web applications (CVE-2008-4104).\n\nThe CVEs related to HP-UX are not directly relevant to the phishing scenario described.\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"# ISO 27001-style incident report generation\nuser_question = \"\"\"\nUsing the context from the threat intelligence index and MITRE ATT&CK techniques,\ngenerate an ISO 27001-style incident report for the following situation:\n\n\"I got a link from an unknown person. After clicking it and downloading a file,\nmy PC doesn’t start properly anymore.\"\n\nStructure the report using typical ISO 27001 incident management sections:\n1. Incident identification and summary\n2. Scope and impact (assets, data, users, business processes)\n3. Cause and attack description (map to relevant MITRE ATT&CK techniques where possible)\n4. Containment actions taken / recommended\n5. Eradication and recovery steps\n6. Lessons learned and preventive controls (policies, training, technical controls)\n7. References to any relevant CVEs or incidents from the context\n\nBase everything ONLY on the retrieved context and MITRE ATT&CK information. If\nsomething is not supported by the context, say that explicitly instead of guessing.\n\"\"\".strip()\n\nask_rag(user_question)","metadata":{"execution":{"iopub.status.busy":"2026-02-10T14:22:44.458479Z","iopub.execute_input":"2026-02-10T14:22:44.459417Z","iopub.status.idle":"2026-02-10T14:22:45.916747Z","shell.execute_reply.started":"2026-02-10T14:22:44.459381Z","shell.execute_reply":"2026-02-10T14:22:45.915943Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question:\nUsing the context from the threat intelligence index and MITRE ATT&CK techniques,\ngenerate an ISO 27001-style incident report for the following situation:\n\n\"I got a link from an unknown person. After clicking it and downloading a file,\nmy PC doesn’t start properly anymore.\"\n\nStructure the report using typical ISO 27001 incident management sections:\n1. Incident identification and summary\n2. Scope and impact (assets, data, users, business processes)\n3. Cause and attack description (map to relevant MITRE ATT&CK techniques where possible)\n4. Containment actions taken / recommended\n5. Eradication and recovery steps\n6. Lessons learned and preventive controls (policies, training, technical controls)\n7. References to any relevant CVEs or incidents from the context\n\nBase everything ONLY on the retrieved context and MITRE ATT&CK information. If\nsomething is not supported by the context, say that explicitly instead of guessing.\n\nAnswer:\n1. Incident identification and summary:\nThe incident involves a user receiving a link from an unknown person, downloading a file, and subsequently experiencing a failure to start their PC properly.\n\n2. Scope and impact (assets, data, users, business processes):\nThe scope includes the user's PC and potentially the data stored on it. The impact is on the user's ability to work, potentially affecting business processes that rely on the user's productivity.\n\n3. Cause and attack description:\nThe cause is likely related to the downloaded file. Based on the context, a possible attack vector is a denial of service (DoS) attack, potentially related to CVE-2008-1438, where \"crafted data structures\" triggered the creation of large temporary files, leading to disk space exhaustion. A relevant MITRE ATT&CK technique could be \"Data Destruction\" (T1485) or \"Denial of Service\" (T1498).\n\n4. Containment actions taken / recommended:\nImmediate containment actions could include isolating the affected PC from the network to prevent any potential spread of malware. Recommended actions include running a malware scan using an up-to-date antivirus engine.\n\n5. Eradication and recovery steps:\nEradication steps may involve removing any malware or corrupted files. Recovery steps could include restoring the PC to a previous state or reinstalling the operating system if necessary.\n\n6. Lessons learned and preventive controls:\nLessons learned include the importance of caution when clicking on links from unknown persons and downloading files. Preventive controls could include user training on safe computing practices, implementing robust antivirus measures, and ensuring timely updates of antivirus engines to prevent vulnerabilities like CVE-2008-1438.\n\n7. References to any relevant CVEs or incidents from the context:\nCVE-2008-1438 is relevant as it involves a denial of service due to \"crafted data structures\" causing large temporary files, potentially similar to the incident described.\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"import os\nimport base64\nfrom openai import OpenAI\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Groq OpenAI-compatible client (reuses your existing env vars)\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    base_url=os.environ.get(\"OPENAI_BASE_URL\", \"https://api.groq.com/openai/v1\"),\n)\n\nVISION_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"  # vision-capable Groq model\n\nupload = widgets.FileUpload(accept=\"image/*\", multiple=False)\nbutton = widgets.Button(description=\"Run image test\")\noutput = widgets.Output()\n\ndisplay(upload, button, output)\n\ndef on_click(_):\n    with output:\n        clear_output()\n\n        if not upload.value:\n            print(\"Please upload an image first.\")\n            return\n\n        # --- 1) Get image bytes from FileUpload (value is a tuple of dicts) ---\n        item = upload.value[0]\n        content_bytes = item[\"content\"]\n        image_b64 = base64.b64encode(content_bytes).decode(\"utf-8\")\n\n        # --- 2) First LLM call: describe the image ---\n        user_prompt = \"Describe this image briefly, focusing on any security or risk-related elements if present.\"\n\n        try:\n            img_resp = client.chat.completions.create(\n                model=VISION_MODEL,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a helpful cybersecurity assistant.\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": user_prompt},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/png;base64,{image_b64}\"\n                                },\n                            },\n                        ],\n                    },\n                ],\n                max_tokens=250,\n                temperature=0.4,\n                top_p=0.9,\n            )\n        except Exception as e:\n            print(\"Vision API error:\", e)\n            return\n\n        image_description = img_resp.choices[0].message.content\n        print(\"Image description:\\n\")\n        print(image_description)\n        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n\n        # --- 3) RAG retrieval based on the image description ---\n        if not chunk_texts:\n            print(\"RAG index not available (no chunks). Run indexing cells first.\")\n            return\n\n        # Use your existing hybrid retrieval (BM25 + FAISS)\n        try:\n            rag_results = hybrid_search_rrf(\n                image_description,\n                index,\n                chunk_texts,\n                chunk_meta,\n                model,\n                top_k=CONFIG[\"top_k\"],\n            )\n        except NameError:\n            print(\"hybrid_search_rrf or index/model not defined. Run earlier cells first.\")\n            return\n\n        context = \"\\n\\n---\\n\\n\".join([r[\"chunk\"] for r in rag_results])\n\n        # --- 4) Second LLM call: use RAG context to propose prevention ---\n        rag_question = (\n            \"Given the description of the image and the following threat-intel context, \"\n            \"describe likely threats shown in the image and give concrete prevention \"\n            \"and mitigation advice.\"\n        )\n        rag_user_prompt = build_rag_prompt(\n            context,\n            f\"Image description: {image_description}\\n\\n{rag_question}\",\n        )\n\n        try:\n            rag_resp = client.chat.completions.create(\n                model=VISION_MODEL,\n                messages=[\n                    {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": rag_user_prompt},\n                ],\n                max_tokens=400,\n                temperature=0.4,\n                top_p=0.9,\n            )\n        except Exception as e:\n            print(\"RAG+vision API error:\", e)\n            return\n\n        rag_answer = rag_resp.choices[0].message.content\n        print(\"RAG‑augmented prevention advice:\\n\")\n        print(rag_answer)\n\nbutton.on_click(on_click)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T14:57:34.359340Z","iopub.execute_input":"2026-02-10T14:57:34.360109Z","iopub.status.idle":"2026-02-10T14:57:34.446424Z","shell.execute_reply.started":"2026-02-10T14:57:34.360072Z","shell.execute_reply":"2026-02-10T14:57:34.445472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='image/*', description='Upload')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a7282c09b042cf90b886f4adeac6b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(description='Run image test', style=ButtonStyle())","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1757263c69f45819bc54a2d9d64ad40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9187bcabbd467cb420e189d2739871"}},"metadata":{}}],"execution_count":121},{"cell_type":"markdown","source":"## 8. Export for Your Streamlit App\n\n1. **From Kaggle:** Run all cells, then **Save Version → Quick Save** (or **Save & Run All**).\n2. **Download the index:** In the right panel, open **Output** and download the `rag_index` folder (`faiss.index`, `chunks.json`, `metadata.json`, `config.json`).\n3. **In your local project:** Place `rag_index/` next to your Streamlit app; load with `faiss.read_index(\"rag_index/faiss.index\")`, same `embedding_model` from `config.json` for query encoding.\n\n**Optional:** Add this notebook's output as a **Kaggle Dataset** for your team.","metadata":{}},{"cell_type":"markdown","source":"*(End of notebook.)*","metadata":{}},{"cell_type":"markdown","source":"2. **Download the index:** In the right panel, open **Output** and download the `rag_index` folder (contains `faiss.index`, `chunks.json`, `metadata.json`, `config.json`).\n3. **In your local project:** Place `rag_index/` next to your Streamlit app and load:\n   - `faiss.read_index(\"rag_index/faiss.index\")`\n   - Load `chunks.json` and `metadata.json` for displaying sources.\n   - Use the same `embedding_model` from `config.json` for query encoding.\n\n**Optional:** Add this notebook’s output as a **Kaggle Dataset** so your team can use the same index without re-running.","metadata":{}}]}